"""
Historical LOB Data Processor for Mamba-LOB Pretraining.

Optimized for LARGE datasets (30M+ rows):
- Binary storage (numpy) instead of JSON text (~10x faster read)
- Batch writing (50k rows at a time)
- Memory-mapped file support for training
- Chunked storage for sequential access

Features (40-dim):
  - bid_prices[0:10]: Normalized bid prices relative to mid (scaled)
  - bid_volumes[10:20]: Log-transformed bid volumes
  - ask_prices[20:30]: Normalized ask prices relative to mid (scaled)  
  - ask_volumes[30:40]: Log-transformed ask volumes
"""

import os
import sys
import json
import numpy as np
import zipfile
import logging
from pathlib import Path
from multiprocessing import Pool, cpu_count
from functools import partial
from tqdm import tqdm
from datetime import datetime

# Add project root to path
sys.path.append(str(Path(__file__).parent.parent))

from data.processor import LOBFeatureProcessor
from core.config import MarketConfig

# Setup Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("HistoricalProcessor")

# Constants
LOB_FEATURE_DIM = 40
BATCH_SIZE = 50000  # Write 50k rows at a time


class LOBBook:
    """Maintains a full OrderBook image from snapshots and deltas."""
    __slots__ = ['bids', 'asks', 'last_ts', 'is_initialized']
    
    def __init__(self):
        self.bids = {}
        self.asks = {}
        self.last_ts = 0
        self.is_initialized = False

    def update(self, packet):
        data = packet.get('data', {})
        packet_type = packet.get('type')
        
        if packet_type == 'snapshot':
            self.bids = {float(p): float(q) for p, q in data.get('b', [])}
            self.asks = {float(p): float(q) for p, q in data.get('a', [])}
            self.is_initialized = True
        elif packet_type == 'delta' and self.is_initialized:
            for p, q in data.get('b', []):
                price, qty = float(p), float(q)
                if qty == 0:
                    self.bids.pop(price, None)
                else:
                    self.bids[price] = qty
            for p, q in data.get('a', []):
                price, qty = float(p), float(q)
                if qty == 0:
                    self.asks.pop(price, None)
                else:
                    self.asks[price] = qty
        
        self.last_ts = packet.get('ts', 0)
        return self.is_initialized

    def get_orderbook(self):
        sorted_bids = sorted(self.bids.items(), key=lambda x: x[0], reverse=True)[:10]
        sorted_asks = sorted(self.asks.items(), key=lambda x: x[0])[:10]
        return {
            'bids': [[p, q] for p, q in sorted_bids],
            'asks': [[p, q] for p, q in sorted_asks],
            'ts': self.last_ts
        }


def _process_single_zip(zip_path, market_config):
    """Worker function - returns numpy arrays for efficient storage."""
    features_list = []
    timestamps_list = []
    midprices_list = []
    
    book = LOBBook()
    lob_processor = LOBFeatureProcessor(market_config=market_config)
    
    try:
        with zipfile.ZipFile(zip_path, 'r') as z:
            file_name = z.namelist()[0]
            with z.open(file_name) as f:
                for line in f:
                    try:
                        line_str = line.decode('utf-8').strip()
                        if not line_str:
                            continue
                        packet = json.loads(line_str)
                        
                        if not book.update(packet):
                            continue
                        
                        orderbook = book.get_orderbook()
                        
                        if len(orderbook['bids']) < 10 or len(orderbook['asks']) < 10:
                            continue
                        
                        lob_vector = lob_processor.get_attn_lob_features(orderbook)
                        
                        if len(lob_vector) != LOB_FEATURE_DIM:
                            continue
                        
                        ts_ms = packet.get('ts', packet.get('cts', 0))
                        best_bid = orderbook['bids'][0][0]
                        best_ask = orderbook['asks'][0][0]
                        mid_price = (best_bid + best_ask) / 2
                        
                        features_list.append(lob_vector)
                        timestamps_list.append(ts_ms / 1000.0)
                        midprices_list.append(mid_price)
                        
                    except Exception:
                        continue
                        
    except Exception as e:
        return {'error': f"Error in {os.path.basename(zip_path)}: {str(e)}"}
    
    if not features_list:
        return {'error': f"No valid data in {os.path.basename(zip_path)}"}
    
    return {
        'features': np.array(features_list, dtype=np.float32),
        'timestamps': np.array(timestamps_list, dtype=np.float64),
        'midprices': np.array(midprices_list, dtype=np.float32),
        'count': len(features_list)
    }


def process_historical_data(data_dir="data/raw", output_dir="data/processed", file_pattern="*_ob200.data.zip"):
    """
    Main entry point for parallel LOB feature extraction.
    
    Args:
        data_dir: Directory containing the raw data files
        output_dir: Directory to save processed data
        file_pattern: Glob pattern for finding data files (default: "*_ob200.data.zip")
    
    Outputs:
        - {output_dir}/features.npy: (N, 40) float32 array of LOB features
        - {output_dir}/timestamps.npy: (N,) float64 array of Unix timestamps
        - {output_dir}/midprices.npy: (N,) float32 array of mid prices
        - {output_dir}/metadata.json: Dataset statistics
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Find data files using configurable pattern
    import glob
    zip_files = sorted(glob.glob(os.path.join(data_dir, file_pattern)))
    
    if not zip_files:
        logger.error("No valid zip files found (*_ob200.data.zip)")
        return
    
    logger.info(f"ðŸš€ Found {len(zip_files)} files. Processing on {cpu_count()} cores...")
    
    market_config = MarketConfig()
    
    # Accumulators
    all_features = []
    all_timestamps = []
    all_midprices = []
    total_records = 0
    errors = 0
    
    # Parallel processing
    with Pool(processes=cpu_count()) as pool:
        worker_func = partial(_process_single_zip, market_config=market_config)
        
        for result in tqdm(pool.imap_unordered(worker_func, zip_files), 
                          total=len(zip_files), desc="Extracting features"):
            
            if 'error' in result:
                logger.warning(result['error'])
                errors += 1
                continue
            
            all_features.append(result['features'])
            all_timestamps.append(result['timestamps'])
            all_midprices.append(result['midprices'])
            total_records += result['count']
            
            # Log progress every 1M records
            if total_records % 1_000_000 < BATCH_SIZE:
                logger.info(f"Progress: {total_records:,} records processed")
    
    if not all_features:
        logger.error("No data extracted!")
        return
    
    # Concatenate all arrays
    logger.info("ðŸ“¦ Concatenating arrays...")
    features = np.concatenate(all_features, axis=0)
    timestamps = np.concatenate(all_timestamps, axis=0)
    midprices = np.concatenate(all_midprices, axis=0)
    
    # Sort by timestamp
    logger.info("ðŸ”„ Sorting by timestamp...")
    sort_idx = np.argsort(timestamps)
    features = features[sort_idx]
    timestamps = timestamps[sort_idx]
    midprices = midprices[sort_idx]
    
    # Save to disk
    logger.info("ðŸ’¾ Saving to disk...")
    np.save(output_path / "features.npy", features)
    np.save(output_path / "timestamps.npy", timestamps)
    np.save(output_path / "midprices.npy", midprices)
    
    # Metadata
    metadata = {
        'total_records': int(total_records),
        'feature_dim': LOB_FEATURE_DIM,
        'dtype': 'float32',
        'timestamp_min': float(timestamps[0]),
        'timestamp_max': float(timestamps[-1]),
        'timestamp_min_str': datetime.fromtimestamp(timestamps[0]).isoformat(),
        'timestamp_max_str': datetime.fromtimestamp(timestamps[-1]).isoformat(),
        'duration_hours': float((timestamps[-1] - timestamps[0]) / 3600),
        'files_processed': len(zip_files),
        'errors': errors,
        'created_at': datetime.now().isoformat()
    }
    
    with open(output_path / "metadata.json", 'w') as f:
        json.dump(metadata, f, indent=2)
    
    # Stats
    logger.info(f"âœ¨ Done! Saved {total_records:,} records to {output_dir}/")
    logger.info(f"   Features: {features.shape} ({features.nbytes / 1e9:.2f} GB)")
    logger.info(f"   Time range: {metadata['duration_hours']:.1f} hours")
    logger.info(f"   Errors: {errors} files")
    
    # Free memory explicitly
    logger.info("ðŸ§¹ Cleaning up memory...")
    del features, timestamps, midprices, sort_idx
    del all_features, all_timestamps, all_midprices
    import gc
    gc.collect()
    logger.info("âœ… Memory freed!")
    
    return metadata


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Process historical LOB data for Mamba-LOB pretraining")
    parser.add_argument("--data-dir", "-d", default="data/raw", help="Directory with data files")
    parser.add_argument("--output", "-o", default="data/processed", help="Output directory")
    parser.add_argument("--file-pattern", "-p", default="*_ob200.data.zip", help="Glob pattern for data files")
    
    args = parser.parse_args()
    process_historical_data(data_dir=args.data_dir, output_dir=args.output, file_pattern=args.file_pattern)
